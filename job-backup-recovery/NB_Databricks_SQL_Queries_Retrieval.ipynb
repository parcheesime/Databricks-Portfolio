{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b2f7a07-c9bb-46c7-9083-be88706a0897",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#-----------------------------------------------------------------------Retrieve a lost Databricks SQL Query-------------------------------------------------------------------\n",
    "# Get a sql query to recreate query, search json data to find query using the function get_all_files_from_s3(bucket, folder).\n",
    "# For refrence, all jobs are backed up on Wednesday, use a Wednesday date.\n",
    "# Files are saved accoring to user that created the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b2c165e-b1c2-42b4-a48b-b5d02292cddd",
     "showTitle": true,
     "title": "Libraries"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import date\n",
    "import boto3\n",
    "import os\n",
    "from urllib.parse import urljoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13dcf0e3-66b5-4798-ab3a-e938b58b4453",
     "showTitle": true,
     "title": "Variables"
    }
   },
   "outputs": [],
   "source": [
    "DBATSECRETSCOPE = \"Production-ELT\"\n",
    "DBPATKEY = \"DB_API_Token\"\n",
    "\n",
    "host_token = dbutils.secrets.get(DBATSECRETSCOPE, DBPATKEY)\n",
    "\n",
    "# API key to authenticate with the Databricks REST API\n",
    "headers = {\"Authorization\": \"Bearer \" + host_token}\n",
    "\n",
    "# configure workspace URL\n",
    "host_name = \"<work-place-url>\"\n",
    "\n",
    "# accessing AWS S3 \n",
    "bucket = '<workplace-s3-bucket>'\n",
    "main_folder = 'databricks-queries-settings-weekly'\n",
    "# Initialize the S3 client\n",
    "s3 = boto3.client('s3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b650790-14c6-4daa-b7f8-3db142ee2a99",
     "showTitle": true,
     "title": "Functions"
    }
   },
   "outputs": [],
   "source": [
    "%run \"/PROD/Reuseable_Code/nb_prod_functions_job_backup_rebuild\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68b738b6-f56e-4758-834b-de28b671e9f6",
     "showTitle": true,
     "title": "Get Folder Name: Search Folder with JSON Files"
    }
   },
   "outputs": [],
   "source": [
    "folder_path = f's3a://{bucket}/{main_folder}'\n",
    "file_list = dbutils.fs.ls(folder_path)\n",
    "\n",
    "for subfolder_name in file_list:\n",
    "    print(file.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04d9f48f-8aec-4689-bcbf-628ff78f68ad",
     "showTitle": true,
     "title": "Find Desired Query"
    }
   },
   "outputs": [],
   "source": [
    "subfolder_name = 'aletia_trepte/'\n",
    "subfolder_path = f's3a://{bucket}/{main_folder}/{subfolder_name}'\n",
    "subfolder_files_list = dbutils.fs.ls(subfolder_path)\n",
    "n = 0\n",
    "for subfolder_files in subfolder_files_list:\n",
    "    file_path = subfolder_files.path\n",
    "    df = spark.read.option(\"multiline\", \"true\").json(file_path)\n",
    "    queries = df.select(\"query\").rdd.flatMap(lambda x: x)\n",
    "    for query in queries.collect():\n",
    "        print('**********************************************************')\n",
    "        print(f'                       QUERY {n}                          ')\n",
    "        print('**********************************************************')\n",
    "        if query:  # Check if the query is not null\n",
    "            for line in query.split('\\n'):\n",
    "                print(line)\n",
    "            n = n+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "941b5f6e-e33e-46ab-87ba-c084ef00cae5",
     "showTitle": true,
     "title": "Print/Run Desired Query"
    }
   },
   "outputs": [],
   "source": [
    "subfolder_name = 'aletia_trepte/'\n",
    "subfolder_path = f's3a://{bucket}/{main_folder}/{subfolder_name}'\n",
    "subfolder_files_list = dbutils.fs.ls(subfolder_path)\n",
    "\n",
    "# Variable to store the first query\n",
    "first_query = None\n",
    "n = 0\n",
    "\n",
    "for subfolder_files in subfolder_files_list:\n",
    "    file_path = subfolder_files.path\n",
    "    df = spark.read.option(\"multiline\", \"true\").json(file_path)\n",
    "    queries = df.select(\"query\").rdd.flatMap(lambda x: x)\n",
    "    \n",
    "    for query in queries.collect():\n",
    "        if n == 0 and query:  # Save the first query\n",
    "            first_query = query\n",
    "            break\n",
    "        n += 1\n",
    "    \n",
    "    if first_query is not None:\n",
    "        break  # Exit the outer loop if the first query is found\n",
    "\n",
    "# Print the first query if it exists\n",
    "if first_query:\n",
    "    print('**********************************************************')\n",
    "    print(f'                       QUERY {n}                          ')\n",
    "    print('**********************************************************')\n",
    "    for line in first_query.split('\\n'):\n",
    "        print(line)\n",
    "else:\n",
    "    print(\"No query found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a9fdef9-86df-43f0-ba40-ccf6aeb5cc63",
     "showTitle": true,
     "title": "Run the Query"
    }
   },
   "outputs": [],
   "source": [
    "result_df = spark.sql(first_query)\n",
    "result_df.show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "NB_Databricks_SQL_Queries_Retrieval",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
