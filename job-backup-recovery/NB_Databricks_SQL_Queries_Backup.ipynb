{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d48d8129-a9b7-42ac-a17b-499c569e08ad",
     "showTitle": true,
     "title": "Libraries"
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as t\n",
    "import requests\n",
    "import json\n",
    "import math\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import boto3\n",
    "import os\n",
    "from urllib.parse import urljoin\n",
    "from datetime import date\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "466f4398-638d-48f0-8de4-125d9b666b6b",
     "showTitle": true,
     "title": "Set Up Credentials"
    }
   },
   "outputs": [],
   "source": [
    "DBATSECRETSCOPE = \"Production-ELT\"\n",
    "DBPATKEY = \"DB_API_Token\"\n",
    "\n",
    "host_token = dbutils.secrets.get(DBATSECRETSCOPE, DBPATKEY)\n",
    "\n",
    "API_ENDPOINT = '/api/2.0/preview/sql/queries'\n",
    "\n",
    "# Databricks API key to authenticate with the Databricks REST API\n",
    "headers = {\"Authorization\": \"Bearer \" + host_token}\n",
    "\n",
    "# configure the Databricks workspace URL\n",
    "host_name = \"<work-place-url>\"\n",
    "\n",
    "bucket = '<work-s3-bucket>'\n",
    "folder = 'databricks-queries-settings-weekly/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf88cdbc-2e46-445c-b3aa-36cb8e4ed0c7",
     "showTitle": true,
     "title": "Functions"
    }
   },
   "outputs": [],
   "source": [
    "def get_queries(api_url, token, page_size=25):\n",
    "    all_queries = []\n",
    "    page = 1\n",
    "    total_pages = 1  # Initialize\n",
    "\n",
    "    while page <= total_pages:\n",
    "        params = {\n",
    "            'page': page,\n",
    "            'page_size': page_size\n",
    "        }\n",
    "        headers = {\n",
    "            'Authorization': f'Bearer {token}'\n",
    "        }\n",
    "        response = requests.get(urljoin(host_name, api_url), headers=headers, params=params)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            total_pages = math.ceil(data['count'] / page_size)  # Update total_pages based on response\n",
    "            results = data['results']\n",
    "\n",
    "            for query in results:\n",
    "                if not query['is_draft']:\n",
    "                    all_queries.append(query)\n",
    "\n",
    "            page += 1\n",
    "        else:\n",
    "            # Handle errors\n",
    "            print(f\"Error: {response.status_code}\")\n",
    "            break\n",
    "\n",
    "    return all_queries\n",
    "\n",
    "\n",
    "# save queries to s3 bucket\n",
    "def save_query_data_to_s3(bucket_name, user_name, data):\n",
    "    s3 = boto3.client('s3')\n",
    "\n",
    "    # Get the current timestamp in YYYY-MM-DD_HH-MM-SS-ms format\n",
    "    now = datetime.now().strftime('%Y-%m-%d_%H-%M-%S-%f')\n",
    "\n",
    "    # Check if the user name is available, otherwise use a default folder name\n",
    "    if user_name.lower() == \"unavailable\":\n",
    "        folder_name = \"unknown_users\"\n",
    "    else:\n",
    "        # Format the folder name: lowercase and replace spaces with underscores\n",
    "        folder_name = user_name.replace(\" \", \"_\").lower()\n",
    "\n",
    "    # Define the main folder name\n",
    "    main_folder = 'databricks-queries-settings-weekly'\n",
    "\n",
    "    # Construct the full key with the main folder, user folder, and file name including the timestamp\n",
    "    key = f\"{main_folder}/{folder_name}/query_data_{folder_name}_{now}.json\"\n",
    "    json_data = json.dumps(data, indent=4)\n",
    "\n",
    "    try:\n",
    "        s3.put_object(Bucket=bucket_name, Key=key, Body=json_data, ContentType='application/json')\n",
    "        print(f\"Data for user {user_name} successfully saved to {bucket_name}/{key}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to save data for user {user_name} to S3. Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "418f4364-9d4e-4b13-8038-0bdb2082ea7f",
     "showTitle": true,
     "title": "Save to S3 Bucket"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# save job setting to aws s3 bucket/folder\n",
    "bucket = '<work-s3-bucket>'\n",
    "\n",
    "queried_data = get_queries(API_ENDPOINT, host_token)\n",
    "#save_query_data_to_s3(bucket_name, user_name, data)\n",
    "for data in queried_data:\n",
    "    name = data[\"user\"][\"name\"]\n",
    "    save_query_data_to_s3(bucket, name, queried_data)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "NB_Databricks_SQL_Queries_Backup",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
