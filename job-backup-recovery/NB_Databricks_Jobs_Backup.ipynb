{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87d4c61f-7103-45d4-9976-2a6128f912d2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------BACK UP JOBS IN DATABRICKS--------------------------------------------------------------------------\n",
    "# This script calls the Databricks API and gets a list of current databricks jobs and saves them in the AWS S3 bucket, s3://use1-s3-bcq-prod-elt-logs/databricks-jobs-settings-weekly/. Some of the settings data for each job is also saved in a google sheet, https://docs.google.com/spreadsheets/d/1C_5SaXWytuWL_7GlNVVypMLTrUza6kmn7v746-0omDg/edit#gid=156737717, for a quick view of the jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d48d8129-a9b7-42ac-a17b-499c569e08ad",
     "showTitle": true,
     "title": "Libraries"
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as t\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "466f4398-638d-48f0-8de4-125d9b666b6b",
     "showTitle": true,
     "title": "Set Up Credentials"
    }
   },
   "outputs": [],
   "source": [
    "DBATSECRETSCOPE = \"Production-ELT\"\n",
    "DBPATKEY = \"DB_API_Token\"\n",
    "\n",
    "host_token = dbutils.secrets.get(DBATSECRETSCOPE, DBPATKEY)\n",
    "\n",
    "# Databricks API key to authenticate with the Databricks REST API\n",
    "headers = {\"Authorization\": \"Bearer \" + host_token}\n",
    "\n",
    "# Manually configure the Databricks workspace URL\n",
    "host_name = \"<nbwork-host-name>\"\n",
    "\n",
    "bucket = '<work-s3-bucket>'\n",
    "folder = 'databricks-jobs-settings-weekly/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99fbb3f7-45ba-421b-8562-66b9064cc6c8",
     "showTitle": true,
     "title": "Get Functions"
    }
   },
   "outputs": [],
   "source": [
    "%run \"/PROD/Reuseable_Code/nb_prod_functions_job_backup_rebuild\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61aab79f-7f04-4db6-b42c-bdc02121431f",
     "showTitle": true,
     "title": "Get Job Settings Data"
    }
   },
   "outputs": [],
   "source": [
    "# job ids using the function\n",
    "job_ids = get_jobs_list(host_name, headers)\n",
    "\n",
    "# Retrieve job id settings data\n",
    "jobs_data = retrieve_jobs_data(host_name, headers, job_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "418f4364-9d4e-4b13-8038-0bdb2082ea7f",
     "showTitle": true,
     "title": "Save to S3 Bucket"
    }
   },
   "outputs": [],
   "source": [
    "# save job setting to aws s3 bucket/folder\n",
    "for data in jobs_data:\n",
    "    job_id = data['job_id']\n",
    "    save_data_to_s3(bucket, folder, job_id, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0d78268-5415-4d94-ae29-56ba79cf7727",
     "showTitle": true,
     "title": "Jobs Quick View"
    }
   },
   "outputs": [],
   "source": [
    "# create a spreadsheet with quick overview of jobs in databricks\n",
    "def extract_job_data(jobs):\n",
    "    extracted_data = []\n",
    "\n",
    "    for job in jobs:\n",
    "        job_id = job.get('job_id')\n",
    "        last_run_url = job.get('last_run_url', '')\n",
    "        settings = job.get('settings', {})\n",
    "\n",
    "        name = settings.get('name', '')\n",
    "        \n",
    "        # Extracting schedule with checks\n",
    "        schedule_data = settings.get('schedule', {})\n",
    "        quartz_expr = schedule_data.get('quartz_cron_expression', '')\n",
    "        timezone = schedule_data.get('timezone_id', '')\n",
    "        schedule = quartz_expr + \" \" + timezone if quartz_expr and timezone else ''\n",
    "        \n",
    "        format_type = settings.get('format', '')\n",
    "        \n",
    "        # Extracting notebook paths with checks\n",
    "        tasks = settings.get('tasks', [])\n",
    "        notebooks = [task['notebook_task'].get('notebook_path', '') for task in tasks if 'notebook_task' in task]\n",
    "        \n",
    "        # If no clusters are available for the job, add a default row\n",
    "        clusters = settings.get('job_clusters', [])\n",
    "        if not clusters:\n",
    "            extracted_data.append([job_id, name, schedule, str(notebooks), '', '', '', last_run_url])\n",
    "            continue\n",
    "\n",
    "        # Processing clusters\n",
    "        for cluster in clusters:\n",
    "            try:\n",
    "                new_cluster = cluster.get('new_cluster', {})\n",
    "                node_type = new_cluster.get('node_type_id', '')\n",
    "                spark_version = new_cluster.get('spark_version', '')\n",
    "                aws_attributes = new_cluster.get('aws_attributes', {})\n",
    "                instance_profile_arn = aws_attributes.get('instance_profile_arn', '')\n",
    "\n",
    "                extracted_data.append([str(job_id), name, schedule, str(notebooks), node_type, spark_version, instance_profile_arn, last_run_url])\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing cluster for job_id {job_id}: {e}\")\n",
    "\n",
    "    return pd.DataFrame(extracted_data, columns=['job_id', 'name', 'schedule', 'notebooks', 'node_type', 'spark_version', 'instance_profile_arn', 'last_run_url'])\n",
    "\n",
    "df = extract_job_data(jobs_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22745ccf-4cec-4c89-b944-8c6789f39a5a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03c676bc-8e09-42cf-8bb1-e26328147f32",
     "showTitle": false,
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%run \"/Users/aletia@bondcliq.com/nb_df_googlesheet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "379b38ef-82e4-4488-b27c-66e18b32c89c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# save the quick view to Google Sheet, along with the previous week's quick view\n",
    "# we will always have 2 weeks worth of quick views\n",
    "sheet_id = 'google-sheet-id'\n",
    "worksheet_name_A = 'jobs'\n",
    "worksheet_name_B = 'jobs2'\n",
    "try:\n",
    "    rotate_and_save(df, sheet_id, worksheet_name_A, worksheet_name_B)\n",
    "    print('Google Sheet Jobs Data was successfully rotated and updated for Sheet 2.')\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "NB_Databricks_Jobs_Backup",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
